{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mpt777/CS474/blob/main/lab6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDv5g9AT5HdL"
      },
      "source": [
        "<a \n",
        "href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab6.ipynb\"\n",
        "  target=\"_parent\">\n",
        "  <img\n",
        "    src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
        "    alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cksgAH12XRjV"
      },
      "source": [
        "# Lab 6: Sequence-to-sequence models\n",
        "\n",
        "### Description:\n",
        "For this lab, you will code up the [char-rnn model of Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). This is a recurrent neural network that is trained probabilistically on sequences of characters, and that can then be used to sample new sequences that are like the original.\n",
        "\n",
        "This lab will help you develop several new skills, as well as understand some best practices needed for building large models. In addition, we'll be able to create networks that generate neat text!\n",
        "\n",
        "### Deliverable:\n",
        "- Fill in the code for the RNN (using PyTorch's built-in GRU).\n",
        "- Fill in the training loop\n",
        "- Fill in the evaluation loop. In this loop, rather than using a validation set, you will sample text from the RNN.\n",
        "- Implement your own GRU cell.\n",
        "- Train your RNN on a new domain of text (Star Wars, political speeches, etc. - have fun!)\n",
        "\n",
        "### Grading Standards:\n",
        "- 20% Implementation the RNN\n",
        "- 20% Implementation training loop\n",
        "- 20% Implementation of evaluation loop\n",
        "- 20% Implementation of your own GRU cell\n",
        "- 20% Training of your RNN on a domain of your choice\n",
        "\n",
        "### Tips:\n",
        "- Read through all the helper functions, run them, and make sure you understand what they are doing\n",
        "- At each stage, ask yourself: What should the dimensions of this tensor be? Should its data type be float or int? (int is called `long` in PyTorch)\n",
        "- Don't apply a softmax inside the RNN if you are using an nn.CrossEntropyLoss (this module already applies a softmax to its input).\n",
        "\n",
        "### Example Output:\n",
        "An example of my final samples are shown below (more detail in the\n",
        "final section of this writeup), after 150 passes through the data.\n",
        "Please generate about 15 samples for each dataset.\n",
        "\n",
        "<code>\n",
        "And ifte thin forgision forward thene over up to a fear not your\n",
        "And freitions, which is great God. Behold these are the loss sub\n",
        "And ache with the Lord hath bloes, which was done to the holy Gr\n",
        "And appeicis arm vinimonahites strong in name, to doth piseling \n",
        "And miniquithers these words, he commanded order not; neither sa\n",
        "And min for many would happine even to the earth, to said unto m\n",
        "And mie first be traditions? Behold, you, because it was a sound\n",
        "And from tike ended the Lamanites had administered, and I say bi\n",
        "</code>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2i_QpSsWG4c"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 0: Readings, data loading, and high level training\n",
        "\n",
        "---\n",
        "\n",
        "There is a tutorial here that will help build out scaffolding code, and get an understanding of using sequences in pytorch.\n",
        "\n",
        "* Read the following\n",
        "\n",
        "> * [Pytorch sequence-to-sequence tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) (Take note that you will not be implementing the encoder part of this tutorial.)\n",
        "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7bdZWxvJrsx",
        "outputId": "3d1f8ac5-e9ac-4452-be9d-ac30920b17ac"
      },
      "source": [
        "! wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz' \n",
        "! tar -xzf text_files.tar.gz\n",
        "! pip install unidecode\n",
        "! pip install torch\n",
        "\n",
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        " \n",
        "import pdb\n",
        " \n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "file = unidecode.unidecode(open('./text_files/lotr.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-27 17:30:02--  https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz\n",
            "Resolving piazza.com (piazza.com)... 3.208.138.178, 54.236.183.80, 34.195.100.100, ...\n",
            "Connecting to piazza.com (piazza.com)|3.208.138.178|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-uploads.piazza.com/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz [following]\n",
            "--2021-02-27 17:30:03--  https://cdn-uploads.piazza.com/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz\n",
            "Resolving cdn-uploads.piazza.com (cdn-uploads.piazza.com)... 13.35.90.48, 13.35.90.114, 13.35.90.21, ...\n",
            "Connecting to cdn-uploads.piazza.com (cdn-uploads.piazza.com)|13.35.90.48|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1533290 (1.5M) [application/x-gzip]\n",
            "Saving to: ‘./text_files.tar.gz’\n",
            "\n",
            "./text_files.tar.gz 100%[===================>]   1.46M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-02-27 17:30:03 (31.6 MB/s) - ‘./text_files.tar.gz’ saved [1533290/1533290]\n",
            "\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.7.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "file_len = 2579888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxBeKeNjJ0NQ",
        "outputId": "e467a4d0-519d-4a13-9c1f-58a0b74f6ebf"
      },
      "source": [
        "chunk_len = 200\n",
        " \n",
        "def random_chunk():\n",
        "  start_index = random.randint(0, file_len - chunk_len)\n",
        "  end_index = start_index + chunk_len + 1\n",
        "  return file[start_index:end_index]\n",
        "  \n",
        "print(random_chunk())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nging of swords: such sounds as had not been heard in the hallowed \n",
            "places since the building of the City. At last they came to Rath Dnnen and \n",
            "hastened towards the House of the Stewards, looming in th\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "On0_WitWJ99e",
        "outputId": "6235ecec-5d73-4d13-a94e-c06f9f1deff4"
      },
      "source": [
        "import torch\n",
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "  tensor = torch.zeros(len(string)).long()\n",
        "  for c in range(len(string)):\n",
        "      tensor[c] = all_characters.index(string[c])\n",
        "  return tensor\n",
        "\n",
        "print(char_tensor('abcDEF'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([10, 11, 12, 39, 40, 41])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYJPTLcaYmfI"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: Creating your own GRU cell \n",
        "\n",
        "**(Come back to this later - its defined here so that the GRU will be defined before it is used)**\n",
        "\n",
        "---\n",
        "\n",
        "The cell that you used in Part 1 was a pre-defined Pytorch layer. Now, write your own GRU class using the same parameters as the built-in Pytorch class does.\n",
        "\n",
        "Please try not to look at the GRU cell definition. The answer is right there in the code, and in theory, you could just cut-and-paste it. This bit is on your honor!\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "**DONE:**\n",
        "* Create a custom GRU cell\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aavAv50ZKQ-F"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class GRU(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers):\n",
        "    super(GRU, self).__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.W_ir = nn.Linear(input_size, hidden_size)\n",
        "    self.W_hr = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    self.W_iz = nn.Linear(input_size, hidden_size)\n",
        "    self.W_hz = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    self.W_in = nn.Linear(input_size, hidden_size)\n",
        "    self.W_hn = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.tanh = nn.Tanh()\n",
        "    \n",
        "  def forward(self, inputs, hidden):\n",
        "    # Each layer does the following:\n",
        "    # r_t = sigmoid(W_ir*x_t + b_ir + W_hr*h_(t-1) + b_hr)\n",
        "    # z_t = sigmoid(W_iz*x_t + b_iz + W_hz*h_(t-1) + b_hz)\n",
        "    # n_t = tanh(W_in*x_t + b_in + r_t**(W_hn*h_(t-1) + b_hn))\n",
        "    # h_(t) = (1 - z_t)**n_t + z_t**h_(t-1)\n",
        "    # Where ** is hadamard product (not matrix multiplication, but elementwise multiplication)\n",
        "\n",
        "    r_t = self.sigmoid( W_ir(inputs) + W_hr(hidden) )\n",
        "    z_t = self.sigmoid( W_iz(inputs) + W_hz(hidden) )\n",
        "    n_t = self.tanh( W_in(inputs) + torch.mul( r_t, W_hn(hidden) ) )\n",
        "\n",
        "    outputs, hiddens = torch.mul( (1 - z_t), n_t ) + torch.mul(z_t, hidden)\n",
        "    \n",
        "    return outputs, hiddens\n",
        "  \n"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtXdX-B_WiAY"
      },
      "source": [
        "---\n",
        "\n",
        "##  Part 1: Building a sequence to sequence model\n",
        "\n",
        "---\n",
        "\n",
        "Great! We have the data in a useable form. We can switch out which text file we are reading from, and trying to simulate.\n",
        "\n",
        "We now want to build out an RNN model, in this section, we will use all built in Pytorch pieces when building our RNN class.\n",
        "\n",
        "\n",
        "**TODO:**\n",
        "* Create an RNN class that extends from nn.Module.\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6tNdEnzWj5F"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "    super(RNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "    \n",
        "    # more stuff here...\n",
        "    self.embedding = nn.Embedding(self.input_size, self.hidden_size)\n",
        "    self.gru = GRU(self.hidden_size, self.hidden_size, self.n_layers)\n",
        "    self.linear = nn.Linear(self.hidden_size, self.output_size)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, input_char, hidden):\n",
        "    # by reviewing the documentation, construct a forward function that properly uses the output\n",
        "    # of the GRU\n",
        "\n",
        "    # stuff here\n",
        "    output = self.embedding(input_char).view(1, 1, -1)\n",
        "    output, hidden = self.gru(output, hidden)\n",
        "    output = self.linear(output)\n",
        "    output = self.relu(output)\n",
        "    return output, hidden\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(self.n_layers, 1, self.hidden_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrhXghEPKD-5"
      },
      "source": [
        "def random_training_set():    \n",
        "  chunk = random_chunk()\n",
        "  inp = char_tensor(chunk[:-1])\n",
        "  target = char_tensor(chunk[1:])\n",
        "  return inp, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpiGObbBX0Mr"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 2: Sample text and Training information\n",
        "\n",
        "---\n",
        "\n",
        "We now want to be able to train our network, and sample text after training.\n",
        "\n",
        "This function outlines how training a sequence style network goes. \n",
        "\n",
        "**TODO:**\n",
        "\n",
        "**DONE:**\n",
        "* Fill in the pieces.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ALC3Pf8Kbsi"
      },
      "source": [
        "# NOTE: decoder_optimizer, decoder, and criterion will be defined below as global variables\n",
        "def train(inp, target):\n",
        "  ## initialize hidden layers, set up gradient and loss \n",
        "    # your code here\n",
        "  ## /\n",
        "  decoder_optimizer.zero_grad()\n",
        "  hidden = decoder.init_hidden()\n",
        "  loss = 0\n",
        "  \n",
        "  for feature_char, target_char in zip(inp, target):\n",
        "    \n",
        "    decoded_char, hidden = decoder(feature_char, hidden)\n",
        "    loss += criterion(decoded_char.squeeze(0), target_char.unsqueeze(0))\n",
        "    \n",
        "  loss.backward()\n",
        "  decoder_optimizer.step()\n",
        "  \n",
        "  return loss.item()/len(inp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN06NUu3YRlz"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 3: Sample text and Training information\n",
        "\n",
        "---\n",
        "\n",
        "You can at this time, if you choose, also write out your train loop boilerplate that samples random sequences and trains your RNN. This will be helpful to have working before writing your own GRU class.\n",
        "\n",
        "If you are finished training, or during training, and you want to sample from the network you may consider using the following function. If your RNN model is instantiated as `decoder`then this will probabilistically sample a sequence of length `predict_len`\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "**DONE:**\n",
        "* Fill out the evaluate function to generate text frome a primed string\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-bp-OZ1KjNh"
      },
      "source": [
        "def sample_outputs(output, temperature):\n",
        "    \"\"\"Takes in a vector of unnormalized probability weights and samples a character from the distribution\"\"\"\n",
        "    return torch.multinomial(torch.exp(output / temperature), 1)\n",
        "\n",
        "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
        "  ## initialize hidden state, initialize other useful variables\n",
        "  hidden = decoder.init_hidden()\n",
        "  prime_char = char_tensor(prime_str)\n",
        "\n",
        "  # your code here\n",
        "  ## /\n",
        "  evaluated_char = prime_char[-1]\n",
        "\n",
        "  prediction = prime_str\n",
        "  \n",
        "  for i in range(predict_len):\n",
        "    out, hidden = decoder(evaluated_char, hidden)\n",
        "    probability_distribution = out.data.view(-1).div(temperature).exp()\n",
        "    select_ind = torch.multinomial(probability_distribution, 1)[0]\n",
        "\n",
        "    predicted_char = all_characters[select_ind]\n",
        "    evaluated_char = char_tensor(predicted_char)\n",
        "    prediction += predicted_char\n",
        "\n",
        "  return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du4AGA8PcFEW"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: (Create a GRU cell, requirements above)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFS2bpHSZEU6"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Part 5: Run it and generate some text!\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**TODO:** \n",
        "\n",
        "\n",
        "**DONE:**\n",
        "* Create some cool output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Assuming everything has gone well, you should be able to run the main function in the scaffold code, using either your custom GRU cell or the built in layer, and see output something like this. I trained on the “lotr.txt” dataset, using chunk_length=200, hidden_size=100 for 2000 epochs. These are the results, along with the prime string:\n",
        "\n",
        "---\n",
        "\n",
        " G:\n",
        " \n",
        " Gandalf was decrond. \n",
        "'All have lord you. Forward the road at least walk this is stuff, and \n",
        "went to the long grey housel-winding and kindled side was a sleep pleasuring, I do long \n",
        "row hrough. In  \n",
        "\n",
        " lo:\n",
        " \n",
        " lost death it. \n",
        "'The last of the gatherings and take you,' said Aragorn, shining out of the Gate. \n",
        "'Yes, as you there were remembaused to seen their pass, when? What \n",
        "said here, such seven an the sear \n",
        "\n",
        " lo:\n",
        " \n",
        " low, and frod to keepn \n",
        "Came of their most. But here priced doubtless to an Sam up is \n",
        "masters; he left hor as they are looked. And he could now the long to stout in the right fro horseless of \n",
        "the like \n",
        "\n",
        " I:\n",
        " \n",
        " I had been the \n",
        "in his eyes with the perushed to lest, if then only the ring and the legended \n",
        "of the less of the long they which as the \n",
        "enders of Orcovered and smood, and the p \n",
        "\n",
        " I:\n",
        " \n",
        " I they were not the lord of the hoomes. \n",
        "Home already well from the Elves. And he sat strength, and we \n",
        "housed out of the good of the days to the mountains from his perith. \n",
        "\n",
        "'Yess! Where though as if  \n",
        "\n",
        " Th:\n",
        " \n",
        " There yarden \n",
        "you would guard the hoor might. Far and then may was \n",
        "croties, too began to see the drumbred many line \n",
        "and was then hoard walk and they heart, and the chair of the \n",
        "Ents of way, might was \n",
        "\n",
        " G:\n",
        " \n",
        " Gandalf \n",
        "been lat of less the round of the stump; both and seemed to the trees and perished they \n",
        "lay are speered the less; and the wind the steep and have to she \n",
        "precious. There was in the oonly went \n",
        "\n",
        " wh:\n",
        " \n",
        " which went out of the door. \n",
        "Hull the King and of the The days of his brodo \n",
        "stumbler of the windard was a thing there, then it been shining langing \n",
        "to him poor land. They hands; though they seemed ou \n",
        "\n",
        " ra:\n",
        " \n",
        " rather,' have all the least deather \n",
        "down of the truven beginning to the house of sunk. \n",
        "'Nark shorts of the Eyes of the Gate your great nothing as Eret. \n",
        "'I wander trust horn, and there were not, it  \n",
        "\n",
        " I:\n",
        " \n",
        " I can have no mind \n",
        "together! Where don't may had one may little blung \n",
        "terrible to tales. And turn and Gandalf shall be not to as only the Cattring \n",
        "not stopped great the out them forms. On they she lo \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nXFeCmdKodw"
      },
      "source": [
        "import time\n",
        "n_epochs = 5000\n",
        "print_every = 200\n",
        "plot_every = 10\n",
        "hidden_size = 200\n",
        "n_layers = 1\n",
        "lr = 0.001\n",
        " \n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        " \n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKfozqw-6eqb",
        "outputId": "57116915-c9eb-4693-ddf3-966b96e6d804"
      },
      "source": [
        "# n_epochs = 2000\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  loss_ = train(*random_training_set())       \n",
        "  loss_avg += loss_\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "      print(evaluate('Wh', 100), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "      all_losses.append(loss_avg / plot_every)\n",
        "      loss_avg = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[62.45153594017029 (200 4%) 2.2668]\n",
            "Whil, xode farir so, tue a loud sen 'ser \n",
            "aode sto oug and ewin . I herDt enmengoot, meates eir mathe  \n",
            "\n",
            "[125.10501456260681 (400 8%) 2.2142]\n",
            "Wht, ;ourded to the stit, &oy and and as ofly ree stare \\andons aller, ffor wreter. 'I was allee to fo \n",
            "\n",
            "[188.2317395210266 (600 12%) 2.0372]\n",
            "Wh!' leasine \n",
            "the to this allo\n",
            " a from soind sforther the of the sestist the mopts thing \n",
            "grathes what \n",
            "\n",
            "[250.73269081115723 (800 16%) 1.9768]\n",
            "What we parch \n",
            "and now the shood that a not from there liked to the doow misted that his \n",
            "word to the  \n",
            "\n",
            "[313.2021861076355 (1000 20%) 1.5943]\n",
            "When and not us. It will king. \n",
            "\n",
            "\n",
            "He said nothery of \n",
            "the Alleed under the \n",
            "\n",
            "of mached \n",
            "overs. \n",
            "\n",
            "'Leve \n",
            "\n",
            "[375.53284096717834 (1200 24%) 1.8593]\n",
            "Whe over a him. \n",
            "\n",
            "A lorthed. 'Welvering intode \n",
            "of the wormen \n",
            "purued. ' \n",
            "\n",
            "'A said like enemore, and \n",
            " \n",
            "\n",
            "[437.55659317970276 (1400 28%) 1.4815]\n",
            "Wher that \n",
            "said Iwayter. ' he were was them to the \n",
            "doused and was I sproms and he mast cloant,' she s \n",
            "\n",
            "[500.03400015830994 (1600 32%) 1.7123]\n",
            "Whot \n",
            "lass that >ood at you goars. Well, and they passed. \n",
            "\n",
            "'Stupt of they until to your your \n",
            "them gl \n",
            "\n",
            "[562.1795253753662 (1800 36%) 1.8765]\n",
            "Whe one while helves down out of the roose a sated to a flaper and golder and a stailly verom in the w \n",
            "\n",
            "[624.4612154960632 (2000 40%) 1.8716]\n",
            "Wher only fould \n",
            "here rigther hors to the Willd seast the entworth I with a \n",
            "more up the 7ill great hi \n",
            "\n",
            "[686.596479177475 (2200 44%) 1.7090]\n",
            "Whe it was and steaths of twim of shadow the staning with a did like roaling wide, from that were sper \n",
            "\n",
            "[748.5448336601257 (2400 48%) 1.7068]\n",
            "Where \n",
            "remainly \n",
            "made to gonty \n",
            "left. 'Have not young speckled into in while of >ord the ore had all a \n",
            "\n",
            "[810.5058283805847 (2600 52%) 1.5929]\n",
            "Whe percaw of his have Ores and help of his further in the passt and up as were that Pate that know he \n",
            "\n",
            "[872.2829349040985 (2800 56%) 1.5231]\n",
            "Whairs.' \n",
            "\n",
            "'vancing his my I'm the had first in the shadow, as he gounts alone now. ze a like the will \n",
            "\n",
            "[934.2437958717346 (3000 60%) 1.5344]\n",
            "Whe was \n",
            "7oursely has new \n",
            "the. 'Shill \n",
            "from to some of the way at a chames the rose all the ? and \n",
            "th \n",
            "\n",
            "[996.2927453517914 (3200 64%) 1.8693]\n",
            "What we \n",
            "?ir or the some the bagg of the :now the brooding the doss foot and on, it was they are \n",
            "was  \n",
            "\n",
            "[1058.2894279956818 (3400 68%) 1.6156]\n",
            "Whe turns the stridge, was the xoodElf candin. And his swaid it. \n",
            "\n",
            "The was the \n",
            "treen 4irswent off tSa \n",
            "\n",
            "[1120.9997973442078 (3600 72%) 1.8129]\n",
            "Whe hope all the now. '\n",
            "\n",
            "'I will see not stones. \n",
            "\n",
            "'I do Gandalf had stare decentid all that you were  \n",
            "\n",
            "[1184.0031924247742 (3800 76%) 1.5132]\n",
            "Where some, and \n",
            "feared stared in \n",
            "land up the dread that \n",
            "drink. I will on this spokes, and he good i \n",
            "\n",
            "[1246.8746042251587 (4000 80%) 1.7275]\n",
            "Whe and their put a foarth near there if to mer\u000bage. Scame indeed \n",
            "light southant came and leam, and s \n",
            "\n",
            "[1309.0236251354218 (4200 84%) 1.7095]\n",
            "Wh and anchorsem, and get over the near to a was a long \n",
            "the stall a clims atCut. They east the pouth. \n",
            "\n",
            "[1371.5894448757172 (4400 88%) 1.5507]\n",
            "Whe counsered as a shall came \n",
            "perat huddenly had go, for the strange his far a streat for a right the \n",
            "\n",
            "[1434.08553981781 (4600 92%) 1.4817]\n",
            "What to a hand, I know, and now want would 'it all the guide working and stood cormir of his \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "you \n",
            "\n",
            "[1496.457147359848 (4800 96%) 1.5286]\n",
            "Whel, and in the walk and the look a rate meather I seemed to streacted the \n",
            "road to the should seemed \n",
            "\n",
            "[1558.6696105003357 (5000 100%) 1.4808]\n",
            "Where-lording. I am fale there was since of men the hills and glat. It hastened for the mort, and for  \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee0so6aKJ5L8",
        "outputId": "cb1cb542-5705-418b-de53-033ce7a75494"
      },
      "source": [
        "for i in range(10):\n",
        "  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n",
        "  start = random.randint(0,len(start_strings)-1)\n",
        "  print(start_strings[start])\n",
        "#   all_characters.index(string[c])\n",
        "  print(evaluate(start_strings[start], 200), '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Th\n",
            " The ton to done. They had - on the \n",
            "for uffer to his once. \n",
            "\n",
            "\n",
            "\n",
            "The found out of the darkness of \n",
            "the time. He shoulder passing, \n",
            "and all the dark were may stood, and the way with the ringly in the will  \n",
            "\n",
            " ra\n",
            " ragoring old stoor road this \n",
            "at mon, and this guess \n",
            "the lasts of this folk the treached. \n",
            "\n",
            "Then 2 might. know more they what's it recharth, \n",
            "and tather did and were noits upon their speak while what m \n",
            "\n",
            " G\n",
            " Gall Frodo and do 5or you will slowed to \n",
            "which \n",
            "the morning you can to the on!' Xently the dirs: do not silent on the 3)`k, for Pippin, thought the \n",
            "dright, Denound and \n",
            "his pon to me. I've recasing w \n",
            "\n",
            " wh\n",
            " whe or hope; for the trest would \n",
            "ardel in the little higher would for the moming more the cumpasted from the rasves who were have a twight; and it was ta \n",
            "\n",
            " he\n",
            " her ~imlit. He some, and he \n",
            "have march in the Xessained there was nothing, and going at one with the \n",
            "will falles, he said, and you don't dark than a \n",
            "pires-found they had some or \n",
            "soon at have not wer \n",
            "\n",
            " Th\n",
            " The morning Stronies \n",
            "_race horse more to the looknel, ~atting they were only for him. \n",
            "\n",
            "'I though ray surnonter. \n",
            "\n",
            "'They understands it road. Beforen in \n",
            "were least, if you a ride \n",
            "all to think in the  \n",
            "\n",
            " ca\n",
            " cagon(, you have it more to \n",
            "than that could ?' \n",
            "\n",
            "The \n",
            "treak many glad for a lightfully as Some away you are Rider many still aware: from the and from had \n",
            "mere and my light would diss of I great more m \n",
            "\n",
            " ra\n",
            " rand ?ight, \n",
            "and the \n",
            "marched him. I knowing watchol- and threass; and Sam for may of we were he do not hastes any am`ways them wandel to those were such, in the \n",
            "whole if his land, \n",
            "and peril to littin \n",
            "\n",
            " ca\n",
            " cavce hoor for little that not a gatherous done with things of more to cliff \n",
            "makes much fice more of the egolasion, and which dine \n",
            " away. \n",
            "\n",
            "It short, and seemed quice their on that words of my surpone \n",
            "\n",
            " wh\n",
            " whtain for the great no on the \n",
            "few more for your \n",
            "way more, or foot to alreaking that was from a 'though of their hand in a find. 'If you goling, mind, canto his many had \n",
            "denour sught. There. It more  \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJhgDc2IauPE"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 6: Generate output on a different dataset\n",
        "\n",
        "---\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "\n",
        "**DONE:**\n",
        "* Choose a textual dataset. Here are some [text datasets](https://www.kaggle.com/datasets?tags=14104-text+data%2C13205-text+mining) from Kaggle \n",
        "\n",
        "* Generate some decent looking results and evaluate your model's performance (say what it did well / not so well)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HV03UkQDlfu"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "text = pd.read_csv(\"./Costumes_Amazon.csv\", error_bad_lines=False)\r\n",
        "\r\n",
        "review_text = text.iloc[:,0]\r\n",
        "\r\n",
        "final_text = ' '.join(review_text.tolist())\r\n",
        "\r\n",
        "\r\n",
        "myText = open(r'/reviews.txt','w')\r\n",
        "myText.write(final_text)\r\n",
        "myText.close()"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOPAeeBbI2iG",
        "outputId": "3b4efffc-65c1-4c36-a954-5c9f3d280950"
      },
      "source": [
        "all_characters = string.printable\r\n",
        "n_characters = len(all_characters)\r\n",
        "file = unidecode.unidecode(open('./reviews.txt').read())\r\n",
        "file_len = len(file)\r\n",
        "\r\n",
        "chunk_len = 200\r\n",
        " \r\n",
        "def random_chunk():\r\n",
        "  start_index = random.randint(0, file_len - chunk_len)\r\n",
        "  end_index = start_index + chunk_len + 1\r\n",
        "  return file[start_index:end_index]\r\n",
        "  \r\n",
        "print(random_chunk())\r\n",
        "\r\n",
        "def random_training_set():    \r\n",
        "  chunk = random_chunk()\r\n",
        "  inp = char_tensor(chunk[:-1])\r\n",
        "  target = char_tensor(chunk[1:])\r\n",
        "  return inp, target"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "on my head to fill in space. Out of all the costumes of this style, this is the best.\n",
            " \n",
            "  I ordered this for my husband. He was kind of thrown off by the trim which is a bright gold. But, overall it's \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h542YylTIW-p",
        "outputId": "5bb1731c-9960-437d-d381-74f5011cc528"
      },
      "source": [
        "# n_epochs = 2000\r\n",
        "for epoch in range(1, n_epochs + 1):\r\n",
        "  loss_ = train(*random_training_set())       \r\n",
        "  loss_avg += loss_\r\n",
        "\r\n",
        "  if epoch % print_every == 0:\r\n",
        "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\r\n",
        "      print(evaluate('T', 100), '\\n')\r\n",
        "\r\n",
        "  if epoch % plot_every == 0:\r\n",
        "      all_losses.append(loss_avg / plot_every)\r\n",
        "      loss_avg = 0"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1614462539.893113 (200 4%) 1.2474]\n",
            "The man plain car compantally in the price part to odd and the sleeves and came ealisy good.\n",
            " \n",
            "  They \n",
            "\n",
            "[1614462602.4062767 (400 8%) 1.8041]\n",
            "To did tords perfect and its pero real up and didn't care at a really downsing legs with my 1stame an \n",
            "\n",
            "[1614462665.1837597 (600 12%) 1.0432]\n",
            "THI'ST I ears danding.\n",
            " \n",
            "  Though, noise it car [under this costume for my daughter is super and it w \n",
            "\n",
            "[1614462727.0978496 (800 16%) 1.3695]\n",
            "The costume has spra(s it and they stretchy pieces and it fit perfect from and perfect but still fit  \n",
            "\n",
            "[1614462789.2747843 (1000 20%) 1.2221]\n",
            "To. It's a little ^sall and it fits performy for the colder than a Halloween costume.  My son loved t \n",
            "\n",
            "[1614462851.3359609 (1200 24%) 1.4311]\n",
            "TOK, it's will #lreaded for a 3T fit...\n",
            " \n",
            "  {uck head you my little long and he's I end and it fits m \n",
            "\n",
            "[1614462913.4324393 (1400 28%) 1.5050]\n",
            "The costuments head out so I wear a tail she will <rd On used to too laughter so it was for when is s \n",
            "\n",
            "[1614462975.9026964 (1600 32%) 1.4111]\n",
            "Thing loves the package.It's now that I didn't was she would not would story coming to have the chest \n",
            "\n",
            "[1614463037.766202 (1800 36%) 1.4685]\n",
            "The hit we lish the perfect and con again and it would worked party and our dagges\n",
            " \n",
            "  So minutes and \n",
            "\n",
            "[1614463100.012762 (2000 40%) 1.4409]\n",
            "The Bold store to ; the will is cheap for my pink really real clear and it's the costume y looks grea \n",
            "\n",
            "[1614463162.127503 (2200 44%) 1.4144]\n",
            "The smil and a small <rth it shipped of cute and were every per perfect.  The only I know it was poes \n",
            "\n",
            "[1614463224.246974 (2400 48%) 1.4022]\n",
            "The part is a small and the costume is definitely cute\n",
            " \n",
            "  I wore I was now it. S=LO my feet xl party \n",
            "\n",
            "[1614463286.6861959 (2600 52%) 1.3258]\n",
            "The only compliments on my straps it up the ground the couns. This is very long in it, glad I even we \n",
            "\n",
            "[1614463349.1368985 (2800 56%) 1.6018]\n",
            "This that will warm.\n",
            " \n",
            "  I am super we like the 3-upunicoo& runs can wear a warm gave it to have to's \n",
            "\n",
            "[1614463411.3441155 (3000 60%) 1.1133]\n",
            "This, what it was a great costume was }arge too long the 6laty. The warm glad 1ast and it fit perfect \n",
            "\n",
            "[1614463473.2731926 (3200 64%) 1.2278]\n",
            "The arms with the house it it was a little ; and the over one of girl loves it. It work to make some  \n",
            "\n",
            "[1614463535.0413508 (3400 68%) 1.3361]\n",
            "The {ank to I was a cutfice. The such to go to reflective leg when I was a put it up to her photos. I \n",
            "\n",
            "[1614463597.1114678 (3600 72%) 1.5795]\n",
            "The ordering it is a very compliments, and it was a medium and little little larger than doesn't fall \n",
            "\n",
            "[1614463658.9741406 (3800 76%) 1.2646]\n",
            "This is the funched in a ciles. I am, and is apart is the shorts the =Kinds if you are sometimes. Tha \n",
            "\n",
            "[1614463721.0388172 (4000 80%) 1.3552]\n",
            "The than that is two `uch is to the `ought it really around the house I ordered a good product is wha \n",
            "\n",
            "[1614463782.7799835 (4200 84%) 1.5032]\n",
            "The hat is herstil was holdie providC, I'd daughter look in a doctor ;1-I should have everything in t \n",
            "\n",
            "[1614463844.401705 (4400 88%) 1.0840]\n",
            "The /O). I enchice is a costume, even if you can part at a large height it and is powife when I mank  \n",
            "\n",
            "[1614463906.2581084 (4600 92%) 1.2995]\n",
            "TEKard with all over of a little 5 stars in a frown costume for well made on it, outfit is en a fuine \n",
            "\n",
            "[1614463968.0568342 (4800 96%) 1.2928]\n",
            "This decided moments on it on it.  I am an ited it and she our so stroom so it looks gereas or set an \n",
            "\n",
            "[1614464029.9014049 (5000 100%) 1.7133]\n",
            "T1\\] a little princess costume ever... It is with the only the shight in gets greath.\n",
            " \n",
            "  I loved it  \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-geon37FmPg3"
      },
      "source": [
        "For my text, I chose a kaggle dataset of Amazon reviews for halloween costumes. Very fun to play around with! \r\n",
        "\r\n",
        "The model was able to produce fairly good results! There still were issues with random characters, spelling, and symbols, but the setiment was definitely correct! I think the final line of the model \"I loved it\" was the perfect way to close off the project "
      ]
    }
  ]
}